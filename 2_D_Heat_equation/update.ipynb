{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\adith\\anaconda3\\lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.models import Sequential\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Generate training data\n",
    "np.random.seed(42)\n",
    "num_samples = 1000\n",
    "x_train = np.random.uniform(low=0, high=1, size=(num_samples, 1))\n",
    "t_train = np.random.uniform(low=0, high=1, size=(num_samples, 1))\n",
    "u_exact = np.sin(np.pi * x_train) * np.exp(-np.pi**2 * t_train)\n",
    "\n",
    "# Convert to TensorFlow tensors\n",
    "x_train_tf = tf.convert_to_tensor(x_train, dtype=tf.float32)\n",
    "t_train_tf = tf.convert_to_tensor(t_train, dtype=tf.float32)\n",
    "u_exact_tf = tf.convert_to_tensor(u_exact, dtype=tf.float32)\n",
    "\n",
    "# Combine x and t for training input\n",
    "input_train = tf.concat([x_train_tf, t_train_tf], axis=1)\n",
    "\n",
    "# Generate test data for prediction\n",
    "x_test = np.linspace(0, 1, 100).reshape(-1, 1)\n",
    "t_test = np.linspace(0, 1, 100).reshape(-1, 1)\n",
    "\n",
    "# Convert to TensorFlow tensors\n",
    "x_test_tf = tf.convert_to_tensor(x_test, dtype=tf.float32)\n",
    "t_test_tf = tf.convert_to_tensor(t_test, dtype=tf.float32)\n",
    "input_test = tf.concat([x_test_tf, t_test_tf], axis=1)\n",
    "u_exact_test = np.sin(np.pi * x_test) * np.exp(-np.pi**2 * t_test)\n",
    "u_exact_test_tf = tf.convert_to_tensor(u_exact_test, dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\adith\\anaconda3\\lib\\site-packages\\keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Define the neural network model\n",
    "model = Sequential([\n",
    "    Dense(units=50, activation='tanh'),\n",
    "    Dense(units=50, activation='tanh'),\n",
    "    Dense(units=1, activation='linear')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_alpha = tf.Variable(0.00001, dtype=tf.float32)\n",
    "\n",
    "# Define the loss function\n",
    "def loss(model, x, t, alpha):\n",
    "    with tf.GradientTape(persistent=True) as tape:\n",
    "        tape.watch(x)\n",
    "        tape.watch(t)\n",
    "        u_pred = model(tf.concat((x, t), axis=1))\n",
    "        du_x = tape.gradient(u_pred, x)\n",
    "        du_xx = tape.gradient(du_x, x)\n",
    "        du_dt = tape.gradient(u_pred, t)\n",
    "        del tape\n",
    "\n",
    "    physics_residual = du_dt - alpha * du_xx\n",
    "    physics_loss = tf.reduce_mean(tf.square(physics_residual))\n",
    "    data_loss = tf.reduce_mean(tf.square(u_pred - u_exact_tf))\n",
    "    total_loss = physics_loss + data_loss\n",
    "    \n",
    "    return total_loss, physics_loss, data_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Calling GradientTape.gradient on a persistent tape inside its context is significantly less efficient than calling it outside the context (it causes the gradient ops to be recorded on the tape, leading to increased CPU and memory usage). Only call GradientTape.gradient inside the context if you actually want to trace the gradient in order to compute higher order derivatives.\n",
      "WARNING:tensorflow:Calling GradientTape.gradient on a persistent tape inside its context is significantly less efficient than calling it outside the context (it causes the gradient ops to be recorded on the tape, leading to increased CPU and memory usage). Only call GradientTape.gradient inside the context if you actually want to trace the gradient in order to compute higher order derivatives.\n",
      "WARNING:tensorflow:Calling GradientTape.gradient on a persistent tape inside its context is significantly less efficient than calling it outside the context (it causes the gradient ops to be recorded on the tape, leading to increased CPU and memory usage). Only call GradientTape.gradient inside the context if you actually want to trace the gradient in order to compute higher order derivatives.\n",
      "Epoch 0/2000, Total Loss: 0.04439772665500641, Physics Loss: 0.023873697966337204, Data Loss: 0.020524026826024055\n",
      "Epoch 100/2000, Total Loss: 0.019875263795256615, Physics Loss: 0.0006349727045744658, Data Loss: 0.01924029178917408\n",
      "Epoch 200/2000, Total Loss: 0.01598675362765789, Physics Loss: 0.000977875548414886, Data Loss: 0.015008878894150257\n",
      "Epoch 300/2000, Total Loss: 0.01297401450574398, Physics Loss: 0.0008847829885780811, Data Loss: 0.012089231982827187\n",
      "Epoch 400/2000, Total Loss: 0.011781474575400352, Physics Loss: 0.00044486665865406394, Data Loss: 0.011336607858538628\n",
      "Epoch 500/2000, Total Loss: 0.010818644426763058, Physics Loss: 0.00022930103295948356, Data Loss: 0.01058934349566698\n",
      "Epoch 600/2000, Total Loss: 0.010245248675346375, Physics Loss: 0.0003886642516590655, Data Loss: 0.00985658448189497\n",
      "Epoch 700/2000, Total Loss: 0.0091127073392272, Physics Loss: 0.0002756241010501981, Data Loss: 0.008837083354592323\n",
      "Epoch 800/2000, Total Loss: 0.00822842214256525, Physics Loss: 0.00043343636207282543, Data Loss: 0.007794985547661781\n",
      "Epoch 900/2000, Total Loss: 0.007232450880110264, Physics Loss: 0.00045685606892220676, Data Loss: 0.006775594782084227\n",
      "Epoch 1000/2000, Total Loss: 0.006198744755238295, Physics Loss: 0.00047604835708625615, Data Loss: 0.005722696427255869\n",
      "Epoch 1100/2000, Total Loss: 0.005588109139353037, Physics Loss: 0.0006099046440795064, Data Loss: 0.004978204611688852\n",
      "Epoch 1200/2000, Total Loss: 0.00516153872013092, Physics Loss: 0.0006099760066717863, Data Loss: 0.004551562946289778\n",
      "Epoch 1300/2000, Total Loss: 0.004826793447136879, Physics Loss: 0.0006212352309376001, Data Loss: 0.004205557983368635\n",
      "Epoch 1400/2000, Total Loss: 0.004354264121502638, Physics Loss: 0.0005368476849980652, Data Loss: 0.0038174164947122335\n",
      "Epoch 1500/2000, Total Loss: 0.0040617045015096664, Physics Loss: 0.0005406906711868942, Data Loss: 0.0035210137721151114\n",
      "Epoch 1600/2000, Total Loss: 0.0037846032064408064, Physics Loss: 0.0005161328590475023, Data Loss: 0.003268470289185643\n",
      "Epoch 1700/2000, Total Loss: 0.003521130420267582, Physics Loss: 0.0004638647078536451, Data Loss: 0.003057265654206276\n",
      "Epoch 1800/2000, Total Loss: 0.003228722373023629, Physics Loss: 0.00040567124960944057, Data Loss: 0.0028230510652065277\n",
      "Epoch 1900/2000, Total Loss: 0.00297739589586854, Physics Loss: 0.00036996195558458567, Data Loss: 0.0026074338238686323\n"
     ]
    }
   ],
   "source": [
    "# Define the optimizer\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=1e-3)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 2000\n",
    "for epoch in range(num_epochs):\n",
    "    with tf.GradientTape() as tape:\n",
    "        total_loss, physics_loss_value, data_loss_value = loss(model, x_train_tf, t_train_tf, initial_alpha)\n",
    "        \n",
    "    gradients = tape.gradient(total_loss, model.trainable_variables + [initial_alpha])\n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables + [initial_alpha]))\n",
    "\n",
    "    if epoch % 100 == 0:\n",
    "        print(f\"Epoch {epoch}/{num_epochs}, Total Loss: {total_loss.numpy()}, Physics Loss: {physics_loss_value.numpy()}, Data Loss: {data_loss_value.numpy()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error, mean_absolute_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error: 0.001226570806466043\n",
      "Mean Absolute Error: 0.02545933797955513\n"
     ]
    }
   ],
   "source": [
    "# Prediction\n",
    "u_pred = model(tf.concat([input_test], axis=1))\n",
    "\n",
    "# Calculate Mean Squared Error\n",
    "mse = mean_squared_error(u_exact_test_tf, u_pred)\n",
    "print(f\"Mean Squared Error: {mse}\")\n",
    "\n",
    "# Calculate Mean Absolute Error\n",
    "mae = mean_absolute_error(u_exact_test_tf, u_pred)\n",
    "print(f\"Mean Absolute Error: {mae}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
